{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8D2mabhtCeh"
      },
      "source": [
        "# Title of Your Group Project\n",
        "\n",
        "This Python notebook serves as a template for your group project for the course \"Modeling in Cognitive Science\".\n",
        "\n",
        "This is the practical part of the group project where you get to implement the computational modeling workflow. In this part, you are expected to:\n",
        "\n",
        "\n",
        "*   Implement at least two computational models relevant for your hypothesis. *(3 points)*\n",
        "*   Simulate behavior from the two models. *(3 points)*\n",
        "*   Implement a procedure for fitting the models to data. *(4 points)*\n",
        "*   Implement a procedure for parameter recovery. *(5 points)*\n",
        "*   (Implement a procedure for model recovery.) *(optional; 2 bonus points)*\n",
        "*   Implement a model comparison. *(5 points)*.\n",
        "\n",
        "You can gain a total of 20 points for the practical part of the group project.\n",
        "\n",
        "**Note:** *Some of the exercises below (e.g. Model Simulation) rely on code from previous exercises (e.g., Model Implementation). In such cases, you are encouraged to rely on functions implemented for previous exercises. That is, you don't have to produce redundant code.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HceMyA8DtIZ3"
      },
      "source": [
        "## Model Implementation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Implement and simulate data from two* models that are suitable to test your hypothesis. *(3 points)*\n",
        "\n",
        "<font size=2>*You may implement more than two models if you wish. However, two models are sufficient for this group project.</font>\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "isMmbQsKwZ_z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# YOUR MODEL IMPLEMENTATION CODE GOES HERE\n",
        "\n",
        "# environment of the experiment\n",
        "class TowStepEnv:\n",
        "    # constructor\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "        self.action_space = [0, 1]\n",
        "        self.state_space = [0, 1, 2]\n",
        "        self.transition_prob = 0.7\n",
        "        self.reward = 1\n",
        "        self.terminal = False\n",
        "        self.info = {}\n",
        "        \n",
        "        # matrix of transition probabilities\n",
        "        # 0(action left) -> [0(stay in 0), p(go to 1), 1-p(go to 2)]\n",
        "        # 1(action right) -> [0(stay in 0), 1-p(go to 1), p(go to 2)]\n",
        "        self.stage_1_transition_matrix = np.array([[0, self.transition_prob, 1 - self.transition_prob], # action left\n",
        "                                           [0, 1 - self.transition_prob, self.transition_prob]]) # action right\n",
        "        \n",
        "        self.seed = 0\n",
        "        np.random.seed(self.seed)\n",
        "        self.min_reward_prob = 0.25\n",
        "        self.max_reward_prob = 0.75\n",
        "        # matrix of reward probabilities\n",
        "        # 0(state 0) -> [0 (left), 0(right)]\n",
        "        # 1(state 1) -> [p1 (left), p2(right)]\n",
        "        # 2(state 2) -> [p3 (left), p4(right)]\n",
        "        p1 = np.random.uniform(self.min_reward_prob, self.max_reward_prob) \n",
        "        p2 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        p3 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        p4 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        self.reward_prob_matrix = np.array([[0, 0], # first stage (state 0) for both actions\n",
        "                                            [p1, p2], # second stage (state 1) for both actions\n",
        "                                            [p3, p4]]) # second stage (state 2) for both actions\n",
        "        \n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self.terminal = False\n",
        "        self.info = {}\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.terminal:\n",
        "            raise ValueError(\"Episode has already terminated\")\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "\n",
        "        # if in stage 1\n",
        "        if self.state == 0:\n",
        "            reward = self.reward_function(self.state, action) # reward will be 0\n",
        "            self.state = np.random.choice(self.state_space, p=self.stage_1_transition_matrix[action])\n",
        "\n",
        "            self.info[\"common_transition\"] = self.is_common_state(self.state, action)\n",
        "            self.info[\"state_transition_to\"] = self.state\n",
        "            self.info[\"reward_stage_1\"] = reward\n",
        "            self.info[\"action_stage_1\"] = action\n",
        "            # self.info[\"reward_probabilities_stage_1\"] = self.reward_prob_matrix.flatten()\n",
        "        \n",
        "        # if in stage 2\n",
        "        elif self.state in [1,2]:\n",
        "            reward = self.reward_function(self.state, action)\n",
        "            self.terminal = True\n",
        "            self.info[\"reward_stage_2\"] = reward\n",
        "            self.info[\"action_stage_2\"] = action\n",
        "            # self.info[\"reward_probabilities\"] = self.reward_prob_matrix.flatten()\n",
        "            self.info[\"reward_probabilities\"] = self.reward_prob_matrix.flatten()\n",
        "\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"state:{self.state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        \n",
        "        return self.state, reward, self.terminal, self.info\n",
        "    \n",
        "    # @classmethod\n",
        "    def reward_function(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        if state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        # give a reward according to the probability of getting a reward\n",
        "        # for the action taken in the state ( state-action pair )\n",
        "        reward = np.random.uniform() < self.reward_prob_matrix[state][action]\n",
        "        # scale the reward for a costume reward value equal to self.reward\n",
        "        # makes no difference in case self.reward = 1\n",
        "        reward = reward * self.reward\n",
        "        return reward\n",
        "    \n",
        "    def state_transition_function(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        \n",
        "        new_state = None\n",
        "        terminal = False\n",
        "        if state == 0:\n",
        "            new_state = np.random.choice(self.state_space, p=self.stage_1_transition_matrix[action])\n",
        "        elif state in [1,2]:\n",
        "            terminal = True\n",
        "        else:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        return new_state, terminal\n",
        "\n",
        "    def is_common_state(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        if state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        # return self.stage_1_transition_matrix[action, state] >= 0.5\n",
        "        return self.stage_1_transition_matrix[action, state] == np.max(self.stage_1_transition_matrix[action])\n",
        "        \n",
        "# agent / models\n",
        "# one agent who can have different evaluation algos / policies / models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>common_transition</th>\n",
              "      <th>state_transition_to</th>\n",
              "      <th>reward_stage_1</th>\n",
              "      <th>action_stage_1</th>\n",
              "      <th>reward_stage_2</th>\n",
              "      <th>action_stage_2</th>\n",
              "      <th>reward_probabilities</th>\n",
              "      <th>trail_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.5244067519636624, 0.6075946831862...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.5218262806688234, 0.6178596457346...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.532922861487459, 0.62620150391902...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.46909811609160706, 0.642541968805...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.5074175964505685, 0.6792759380525...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.5381748634687615, 0.7093354342721...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.5254335589249702, 0.6983835767318...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.5030468948951283, 0.7080561391783...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.5023423391866619, 0.7187644359416...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.48553082799226316, 0.709775606903...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   common_transition  state_transition_to  reward_stage_1  action_stage_1  \\\n",
              "0               True                    2               0               1   \n",
              "1              False                    2               0               0   \n",
              "2               True                    1               0               0   \n",
              "3              False                    1               0               1   \n",
              "4              False                    2               0               0   \n",
              "5               True                    1               0               0   \n",
              "6               True                    2               0               1   \n",
              "7               True                    2               0               1   \n",
              "8               True                    2               0               1   \n",
              "9               True                    2               0               1   \n",
              "\n",
              "   reward_stage_2  action_stage_2  \\\n",
              "0               0               1   \n",
              "1               1               0   \n",
              "2               0               1   \n",
              "3               0               0   \n",
              "4               1               1   \n",
              "5               1               0   \n",
              "6               1               1   \n",
              "7               1               0   \n",
              "8               1               1   \n",
              "9               1               0   \n",
              "\n",
              "                                reward_probabilities  trail_index  \n",
              "0  [0.0, 0.0, 0.5244067519636624, 0.6075946831862...            0  \n",
              "1  [0.0, 0.0, 0.5218262806688234, 0.6178596457346...            1  \n",
              "2  [0.0, 0.0, 0.532922861487459, 0.62620150391902...            2  \n",
              "3  [0.0, 0.0, 0.46909811609160706, 0.642541968805...            3  \n",
              "4  [0.0, 0.0, 0.5074175964505685, 0.6792759380525...            4  \n",
              "5  [0.0, 0.0, 0.5381748634687615, 0.7093354342721...            5  \n",
              "6  [0.0, 0.0, 0.5254335589249702, 0.6983835767318...            6  \n",
              "7  [0.0, 0.0, 0.5030468948951283, 0.7080561391783...            7  \n",
              "8  [0.0, 0.0, 0.5023423391866619, 0.7187644359416...            8  \n",
              "9  [0.0, 0.0, 0.48553082799226316, 0.709775606903...            9  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from datetime import datetime\n",
        "\n",
        "# simulate data \n",
        "# (for now from randome agent, as test the environment and task implementation)\n",
        "def simulate_tow_step_task(env:TowStepEnv, agent=None, trails=200):\n",
        "    env.reset()\n",
        "    task_data = {}\n",
        "\n",
        "    sd_for_random_walk = 0.025\n",
        "    time_step = 0\n",
        "    while time_step < trails:\n",
        "        # first stage choice\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            action = np.random.choice([0,1]) # randome agent\n",
        "            s, r, terminal, info = env.step(action)\n",
        "        task_data[time_step] = info\n",
        "        env.reset()\n",
        "        # randome walk for the reward probabilies ( only for the 2 terminal states )\n",
        "        env.reward_prob_matrix[1:] = random_walk_gaussian(env.reward_prob_matrix[1:], sd_for_random_walk)\n",
        "        time_step += 1\n",
        "\n",
        "    return task_data\n",
        "\n",
        "def random_walk_gaussian(prob, sd):\n",
        "    new_prob = prob + np.random.normal(scale = sd, size=np.shape(prob))\n",
        "    return new_prob\n",
        "\n",
        "\n",
        "\n",
        "task_data = simulate_tow_step_task(env=TowStepEnv(), trails=10)\n",
        "\n",
        "task_df = pd.DataFrame.from_dict(task_data, orient='index')\n",
        "task_df['trail_index'] = task_df.index\n",
        "\n",
        "display(task_df)\n",
        "\n",
        "time_identifier = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "task_df.to_csv(f\"task_data_{time_identifier}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'common_transition': True, 'state_transition_to': 2, 'reward_stage_1': 0, 'action_stage_1': 1, 'reward_stage_2': 0, 'action_stage_2': 1, 'reward_probabilities': array([0.        , 0.        , 0.52440675, 0.60759468, 0.55138169,\n",
            "       0.52244159])}\n"
          ]
        }
      ],
      "source": [
        "# simple one run test\n",
        "# np.random.seed(0)\n",
        "env = TowStepEnv()\n",
        "\n",
        "terminal = False\n",
        "while not terminal:\n",
        "    action = np.random.choice([0,1])\n",
        "    s, r, terminal, info = env.step(action)\n",
        "\n",
        "print(env.info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twTHrAfL2PC_"
      },
      "source": [
        "## Model Simulation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Simulate data from both models for a single set of parameters. The simulation should mimic the experiment you are trying to model. *(2 points)*\n",
        "\n",
        "*   Plot the simulated behavior of both models. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIk8efpKv-m-"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL SIMULATION CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VxwTW9LwnvJ"
      },
      "source": [
        "## Parameter Fitting *(4 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable parameter search space *(1 point)*\n",
        "\n",
        "*   Implement a procedure to evaluate the fit of a model based on data *(2 points)*\n",
        "\n",
        "*   Implement a procedure for searching the parameter space. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5OKVYszx7tQ"
      },
      "outputs": [],
      "source": [
        "# YOUR PARAMETER FITTING CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZgrw_ByFsF"
      },
      "source": [
        "## Parameter Recovery *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable space of parameters relevant for parameter recovery *(1 point)*\n",
        "\n",
        "*   Use the functions above to generate behavior from a models, for a given set of (randomly sampled) parameters, and then fit the model to its generated data. Make sure to evaluate the parameter fit in a quantiative manner. *(3 points)*\n",
        "\n",
        "*   Plot the parameter recovery results for both models. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdarAD8yXwN"
      },
      "outputs": [],
      "source": [
        "# YOUR PARAMETER RECOVERY CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naQNlDBfzjnG"
      },
      "source": [
        "## *Optional*: Model Recovery *(2 bonus points)*\n",
        "\n",
        "In this bonus exercise, you may examine model reovery. The bonus points count towards your total group project points. That is, you may accumlate up to 22 points in the practical part of the group project.\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIR51ujwziTM"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL RECOVERY CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q4dfJ7O0BpW"
      },
      "source": [
        "## Model Comparison *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Load and (potentially) preprocess the experimental data. (1 point)\n",
        "\n",
        "*   Fit the two models to the data.  *(1 point)*\n",
        "\n",
        "*   Evaluate which model performs better, taking into account fit and model complexity. *(2 points)*\n",
        "\n",
        "*   Plot the behavior of the winning model against the data. *(1 point)**\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wAGI-kd1yRb"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL COMPARISON CODE GOES HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
